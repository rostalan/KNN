{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.1111111111111112,
  "eval_steps": 500,
  "global_step": 3750,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.002962962962962963,
      "grad_norm": 0.41994500160217285,
      "learning_rate": 0.00019986666666666668,
      "loss": 2.7905,
      "step": 10
    },
    {
      "epoch": 0.005925925925925926,
      "grad_norm": 0.48483729362487793,
      "learning_rate": 0.00019971851851851853,
      "loss": 2.8007,
      "step": 20
    },
    {
      "epoch": 0.008888888888888889,
      "grad_norm": 0.41904065012931824,
      "learning_rate": 0.00019957037037037037,
      "loss": 2.7389,
      "step": 30
    },
    {
      "epoch": 0.011851851851851851,
      "grad_norm": 0.4288961589336395,
      "learning_rate": 0.00019942222222222222,
      "loss": 2.7297,
      "step": 40
    },
    {
      "epoch": 0.014814814814814815,
      "grad_norm": 0.4558022916316986,
      "learning_rate": 0.00019927407407407407,
      "loss": 2.7369,
      "step": 50
    },
    {
      "epoch": 0.017777777777777778,
      "grad_norm": 0.5271287560462952,
      "learning_rate": 0.00019912592592592594,
      "loss": 2.7326,
      "step": 60
    },
    {
      "epoch": 0.02074074074074074,
      "grad_norm": 0.45929065346717834,
      "learning_rate": 0.0001989777777777778,
      "loss": 2.7414,
      "step": 70
    },
    {
      "epoch": 0.023703703703703703,
      "grad_norm": 0.4923892319202423,
      "learning_rate": 0.00019882962962962963,
      "loss": 2.7446,
      "step": 80
    },
    {
      "epoch": 0.02666666666666667,
      "grad_norm": 0.47984567284584045,
      "learning_rate": 0.00019868148148148148,
      "loss": 2.732,
      "step": 90
    },
    {
      "epoch": 0.02962962962962963,
      "grad_norm": 0.441996693611145,
      "learning_rate": 0.00019853333333333335,
      "loss": 2.6847,
      "step": 100
    },
    {
      "epoch": 0.03259259259259259,
      "grad_norm": 0.4086739718914032,
      "learning_rate": 0.00019838518518518517,
      "loss": 2.7017,
      "step": 110
    },
    {
      "epoch": 0.035555555555555556,
      "grad_norm": 0.48233312368392944,
      "learning_rate": 0.00019823703703703704,
      "loss": 2.7306,
      "step": 120
    },
    {
      "epoch": 0.03851851851851852,
      "grad_norm": 0.47894594073295593,
      "learning_rate": 0.0001980888888888889,
      "loss": 2.6507,
      "step": 130
    },
    {
      "epoch": 0.04148148148148148,
      "grad_norm": 0.44831931591033936,
      "learning_rate": 0.00019794074074074076,
      "loss": 2.6891,
      "step": 140
    },
    {
      "epoch": 0.044444444444444446,
      "grad_norm": 0.4817024767398834,
      "learning_rate": 0.00019779259259259258,
      "loss": 2.7207,
      "step": 150
    },
    {
      "epoch": 0.047407407407407405,
      "grad_norm": 0.4535689353942871,
      "learning_rate": 0.00019764444444444446,
      "loss": 2.6558,
      "step": 160
    },
    {
      "epoch": 0.05037037037037037,
      "grad_norm": 0.47607019543647766,
      "learning_rate": 0.0001974962962962963,
      "loss": 2.6613,
      "step": 170
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 0.47628307342529297,
      "learning_rate": 0.00019734814814814815,
      "loss": 2.7113,
      "step": 180
    },
    {
      "epoch": 0.056296296296296296,
      "grad_norm": 0.5075837969779968,
      "learning_rate": 0.0001972,
      "loss": 2.7021,
      "step": 190
    },
    {
      "epoch": 0.05925925925925926,
      "grad_norm": 0.4596754312515259,
      "learning_rate": 0.00019705185185185187,
      "loss": 2.6382,
      "step": 200
    },
    {
      "epoch": 0.06222222222222222,
      "grad_norm": 0.4244016408920288,
      "learning_rate": 0.00019690370370370372,
      "loss": 2.6872,
      "step": 210
    },
    {
      "epoch": 0.06518518518518518,
      "grad_norm": 0.5293035507202148,
      "learning_rate": 0.00019675555555555556,
      "loss": 2.6412,
      "step": 220
    },
    {
      "epoch": 0.06814814814814815,
      "grad_norm": 0.48665326833724976,
      "learning_rate": 0.0001966074074074074,
      "loss": 2.6532,
      "step": 230
    },
    {
      "epoch": 0.07111111111111111,
      "grad_norm": 0.40672728419303894,
      "learning_rate": 0.00019645925925925928,
      "loss": 2.6063,
      "step": 240
    },
    {
      "epoch": 0.07407407407407407,
      "grad_norm": 0.4186539649963379,
      "learning_rate": 0.0001963111111111111,
      "loss": 2.6717,
      "step": 250
    },
    {
      "epoch": 0.07703703703703704,
      "grad_norm": 0.43085652589797974,
      "learning_rate": 0.00019616296296296297,
      "loss": 2.6586,
      "step": 260
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.49082639813423157,
      "learning_rate": 0.00019601481481481482,
      "loss": 2.6642,
      "step": 270
    },
    {
      "epoch": 0.08296296296296296,
      "grad_norm": 0.4273771047592163,
      "learning_rate": 0.00019586666666666667,
      "loss": 2.6523,
      "step": 280
    },
    {
      "epoch": 0.08592592592592592,
      "grad_norm": 0.5517789721488953,
      "learning_rate": 0.0001957185185185185,
      "loss": 2.6184,
      "step": 290
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 0.42642858624458313,
      "learning_rate": 0.00019557037037037039,
      "loss": 2.6667,
      "step": 300
    },
    {
      "epoch": 0.09185185185185185,
      "grad_norm": 0.503442108631134,
      "learning_rate": 0.00019542222222222223,
      "loss": 2.6537,
      "step": 310
    },
    {
      "epoch": 0.09481481481481481,
      "grad_norm": 0.4431065022945404,
      "learning_rate": 0.00019527407407407408,
      "loss": 2.6376,
      "step": 320
    },
    {
      "epoch": 0.09777777777777778,
      "grad_norm": 0.41785353422164917,
      "learning_rate": 0.00019512592592592592,
      "loss": 2.6549,
      "step": 330
    },
    {
      "epoch": 0.10074074074074074,
      "grad_norm": 0.5132063627243042,
      "learning_rate": 0.0001949777777777778,
      "loss": 2.6477,
      "step": 340
    },
    {
      "epoch": 0.1037037037037037,
      "grad_norm": 0.48970046639442444,
      "learning_rate": 0.00019482962962962962,
      "loss": 2.6567,
      "step": 350
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 0.4247705638408661,
      "learning_rate": 0.0001946814814814815,
      "loss": 2.6287,
      "step": 360
    },
    {
      "epoch": 0.10962962962962963,
      "grad_norm": 0.5080249309539795,
      "learning_rate": 0.00019453333333333334,
      "loss": 2.6056,
      "step": 370
    },
    {
      "epoch": 0.11259259259259259,
      "grad_norm": 0.5077942609786987,
      "learning_rate": 0.0001943851851851852,
      "loss": 2.629,
      "step": 380
    },
    {
      "epoch": 0.11555555555555555,
      "grad_norm": 0.4616331160068512,
      "learning_rate": 0.00019423703703703703,
      "loss": 2.6597,
      "step": 390
    },
    {
      "epoch": 0.11851851851851852,
      "grad_norm": 0.5144307613372803,
      "learning_rate": 0.0001940888888888889,
      "loss": 2.6414,
      "step": 400
    },
    {
      "epoch": 0.12148148148148148,
      "grad_norm": 0.44788819551467896,
      "learning_rate": 0.00019394074074074075,
      "loss": 2.6514,
      "step": 410
    },
    {
      "epoch": 0.12444444444444444,
      "grad_norm": 0.4604507386684418,
      "learning_rate": 0.0001937925925925926,
      "loss": 2.6192,
      "step": 420
    },
    {
      "epoch": 0.1274074074074074,
      "grad_norm": 0.5028976798057556,
      "learning_rate": 0.00019364444444444444,
      "loss": 2.5972,
      "step": 430
    },
    {
      "epoch": 0.13037037037037036,
      "grad_norm": 0.5534117221832275,
      "learning_rate": 0.00019349629629629631,
      "loss": 2.598,
      "step": 440
    },
    {
      "epoch": 0.13333333333333333,
      "grad_norm": 0.4733618497848511,
      "learning_rate": 0.00019334814814814816,
      "loss": 2.6438,
      "step": 450
    },
    {
      "epoch": 0.1362962962962963,
      "grad_norm": 0.5088401436805725,
      "learning_rate": 0.0001932,
      "loss": 2.5957,
      "step": 460
    },
    {
      "epoch": 0.13925925925925925,
      "grad_norm": 0.5142859816551208,
      "learning_rate": 0.00019305185185185185,
      "loss": 2.6321,
      "step": 470
    },
    {
      "epoch": 0.14222222222222222,
      "grad_norm": 0.4958926737308502,
      "learning_rate": 0.00019290370370370373,
      "loss": 2.6395,
      "step": 480
    },
    {
      "epoch": 0.1451851851851852,
      "grad_norm": 0.4929180145263672,
      "learning_rate": 0.00019275555555555555,
      "loss": 2.5563,
      "step": 490
    },
    {
      "epoch": 0.14814814814814814,
      "grad_norm": 0.4899669587612152,
      "learning_rate": 0.00019260740740740742,
      "loss": 2.6681,
      "step": 500
    },
    {
      "epoch": 0.1511111111111111,
      "grad_norm": 0.5276218056678772,
      "learning_rate": 0.00019245925925925927,
      "loss": 2.6167,
      "step": 510
    },
    {
      "epoch": 0.15407407407407409,
      "grad_norm": 0.43822163343429565,
      "learning_rate": 0.00019231111111111114,
      "loss": 2.6204,
      "step": 520
    },
    {
      "epoch": 0.15703703703703703,
      "grad_norm": 0.4736675024032593,
      "learning_rate": 0.00019216296296296296,
      "loss": 2.6311,
      "step": 530
    },
    {
      "epoch": 0.16,
      "grad_norm": 0.5057942867279053,
      "learning_rate": 0.00019201481481481483,
      "loss": 2.6546,
      "step": 540
    },
    {
      "epoch": 0.16296296296296298,
      "grad_norm": 0.4702095687389374,
      "learning_rate": 0.00019186666666666668,
      "loss": 2.5997,
      "step": 550
    },
    {
      "epoch": 0.16592592592592592,
      "grad_norm": 0.5003902316093445,
      "learning_rate": 0.00019171851851851852,
      "loss": 2.602,
      "step": 560
    },
    {
      "epoch": 0.1688888888888889,
      "grad_norm": 0.4815703332424164,
      "learning_rate": 0.00019157037037037037,
      "loss": 2.63,
      "step": 570
    },
    {
      "epoch": 0.17185185185185184,
      "grad_norm": 0.49645206332206726,
      "learning_rate": 0.00019142222222222224,
      "loss": 2.6109,
      "step": 580
    },
    {
      "epoch": 0.1748148148148148,
      "grad_norm": 0.42333361506462097,
      "learning_rate": 0.00019127407407407406,
      "loss": 2.6023,
      "step": 590
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 0.46995019912719727,
      "learning_rate": 0.00019112592592592594,
      "loss": 2.6284,
      "step": 600
    },
    {
      "epoch": 0.18074074074074073,
      "grad_norm": 0.48196130990982056,
      "learning_rate": 0.00019097777777777778,
      "loss": 2.5841,
      "step": 610
    },
    {
      "epoch": 0.1837037037037037,
      "grad_norm": 0.4177854359149933,
      "learning_rate": 0.00019082962962962966,
      "loss": 2.5922,
      "step": 620
    },
    {
      "epoch": 0.18666666666666668,
      "grad_norm": 0.4568784236907959,
      "learning_rate": 0.00019068148148148147,
      "loss": 2.6036,
      "step": 630
    },
    {
      "epoch": 0.18962962962962962,
      "grad_norm": 0.5051240921020508,
      "learning_rate": 0.00019053333333333335,
      "loss": 2.5848,
      "step": 640
    },
    {
      "epoch": 0.1925925925925926,
      "grad_norm": 0.5846510529518127,
      "learning_rate": 0.0001903851851851852,
      "loss": 2.6188,
      "step": 650
    },
    {
      "epoch": 0.19555555555555557,
      "grad_norm": 0.5551018118858337,
      "learning_rate": 0.00019023703703703704,
      "loss": 2.64,
      "step": 660
    },
    {
      "epoch": 0.1985185185185185,
      "grad_norm": 0.5218951106071472,
      "learning_rate": 0.0001900888888888889,
      "loss": 2.6144,
      "step": 670
    },
    {
      "epoch": 0.20148148148148148,
      "grad_norm": 0.5011637210845947,
      "learning_rate": 0.00018994074074074076,
      "loss": 2.6236,
      "step": 680
    },
    {
      "epoch": 0.20444444444444446,
      "grad_norm": 0.5580477118492126,
      "learning_rate": 0.0001897925925925926,
      "loss": 2.6138,
      "step": 690
    },
    {
      "epoch": 0.2074074074074074,
      "grad_norm": 0.5741579532623291,
      "learning_rate": 0.00018964444444444445,
      "loss": 2.6092,
      "step": 700
    },
    {
      "epoch": 0.21037037037037037,
      "grad_norm": 0.5032085180282593,
      "learning_rate": 0.0001894962962962963,
      "loss": 2.6511,
      "step": 710
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 0.44381552934646606,
      "learning_rate": 0.00018934814814814817,
      "loss": 2.5687,
      "step": 720
    },
    {
      "epoch": 0.2162962962962963,
      "grad_norm": 0.43698111176490784,
      "learning_rate": 0.0001892,
      "loss": 2.5902,
      "step": 730
    },
    {
      "epoch": 0.21925925925925926,
      "grad_norm": 0.5258404612541199,
      "learning_rate": 0.00018905185185185186,
      "loss": 2.5898,
      "step": 740
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 0.4929201304912567,
      "learning_rate": 0.0001889037037037037,
      "loss": 2.5807,
      "step": 750
    },
    {
      "epoch": 0.22518518518518518,
      "grad_norm": 0.5227887630462646,
      "learning_rate": 0.00018875555555555558,
      "loss": 2.5714,
      "step": 760
    },
    {
      "epoch": 0.22814814814814816,
      "grad_norm": 0.5620380640029907,
      "learning_rate": 0.0001886074074074074,
      "loss": 2.6368,
      "step": 770
    },
    {
      "epoch": 0.2311111111111111,
      "grad_norm": 0.5284554362297058,
      "learning_rate": 0.00018845925925925928,
      "loss": 2.6125,
      "step": 780
    },
    {
      "epoch": 0.23407407407407407,
      "grad_norm": 0.43422773480415344,
      "learning_rate": 0.00018831111111111112,
      "loss": 2.5939,
      "step": 790
    },
    {
      "epoch": 0.23703703703703705,
      "grad_norm": 0.4802998900413513,
      "learning_rate": 0.00018816296296296297,
      "loss": 2.5789,
      "step": 800
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.4693538248538971,
      "learning_rate": 0.00018801481481481482,
      "loss": 2.6057,
      "step": 810
    },
    {
      "epoch": 0.24296296296296296,
      "grad_norm": 0.47105923295021057,
      "learning_rate": 0.0001878666666666667,
      "loss": 2.552,
      "step": 820
    },
    {
      "epoch": 0.24592592592592594,
      "grad_norm": 0.5153153538703918,
      "learning_rate": 0.00018771851851851853,
      "loss": 2.5863,
      "step": 830
    },
    {
      "epoch": 0.24888888888888888,
      "grad_norm": 0.48751240968704224,
      "learning_rate": 0.00018757037037037038,
      "loss": 2.5502,
      "step": 840
    },
    {
      "epoch": 0.2518518518518518,
      "grad_norm": 0.4940889775753021,
      "learning_rate": 0.00018742222222222223,
      "loss": 2.5713,
      "step": 850
    },
    {
      "epoch": 0.2548148148148148,
      "grad_norm": 0.5914392471313477,
      "learning_rate": 0.0001872740740740741,
      "loss": 2.6149,
      "step": 860
    },
    {
      "epoch": 0.2577777777777778,
      "grad_norm": 0.5061697363853455,
      "learning_rate": 0.00018712592592592592,
      "loss": 2.633,
      "step": 870
    },
    {
      "epoch": 0.2607407407407407,
      "grad_norm": 0.4863806664943695,
      "learning_rate": 0.0001869777777777778,
      "loss": 2.5605,
      "step": 880
    },
    {
      "epoch": 0.2637037037037037,
      "grad_norm": 0.5586737394332886,
      "learning_rate": 0.00018682962962962964,
      "loss": 2.5836,
      "step": 890
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.4737390875816345,
      "learning_rate": 0.00018668148148148149,
      "loss": 2.5529,
      "step": 900
    },
    {
      "epoch": 0.2696296296296296,
      "grad_norm": 0.5194921493530273,
      "learning_rate": 0.00018653333333333333,
      "loss": 2.5938,
      "step": 910
    },
    {
      "epoch": 0.2725925925925926,
      "grad_norm": 0.525199294090271,
      "learning_rate": 0.0001863851851851852,
      "loss": 2.5862,
      "step": 920
    },
    {
      "epoch": 0.27555555555555555,
      "grad_norm": 0.5015829205513,
      "learning_rate": 0.00018623703703703705,
      "loss": 2.5813,
      "step": 930
    },
    {
      "epoch": 0.2785185185185185,
      "grad_norm": 0.5682318210601807,
      "learning_rate": 0.0001860888888888889,
      "loss": 2.5738,
      "step": 940
    },
    {
      "epoch": 0.2814814814814815,
      "grad_norm": 0.4906642735004425,
      "learning_rate": 0.00018594074074074074,
      "loss": 2.5658,
      "step": 950
    },
    {
      "epoch": 0.28444444444444444,
      "grad_norm": 0.5127649903297424,
      "learning_rate": 0.00018579259259259262,
      "loss": 2.654,
      "step": 960
    },
    {
      "epoch": 0.2874074074074074,
      "grad_norm": 0.5090605616569519,
      "learning_rate": 0.00018564444444444444,
      "loss": 2.554,
      "step": 970
    },
    {
      "epoch": 0.2903703703703704,
      "grad_norm": 0.49106499552726746,
      "learning_rate": 0.0001854962962962963,
      "loss": 2.5678,
      "step": 980
    },
    {
      "epoch": 0.29333333333333333,
      "grad_norm": 0.5402094721794128,
      "learning_rate": 0.00018534814814814816,
      "loss": 2.5803,
      "step": 990
    },
    {
      "epoch": 0.2962962962962963,
      "grad_norm": 0.4681655168533325,
      "learning_rate": 0.00018520000000000003,
      "loss": 2.5555,
      "step": 1000
    },
    {
      "epoch": 0.2992592592592593,
      "grad_norm": 0.5010385513305664,
      "learning_rate": 0.00018505185185185185,
      "loss": 2.5506,
      "step": 1010
    },
    {
      "epoch": 0.3022222222222222,
      "grad_norm": 0.5298740267753601,
      "learning_rate": 0.00018490370370370372,
      "loss": 2.5961,
      "step": 1020
    },
    {
      "epoch": 0.30518518518518517,
      "grad_norm": 0.49699756503105164,
      "learning_rate": 0.00018475555555555557,
      "loss": 2.5458,
      "step": 1030
    },
    {
      "epoch": 0.30814814814814817,
      "grad_norm": 0.5770794749259949,
      "learning_rate": 0.00018460740740740741,
      "loss": 2.5803,
      "step": 1040
    },
    {
      "epoch": 0.3111111111111111,
      "grad_norm": 0.5048275589942932,
      "learning_rate": 0.00018445925925925926,
      "loss": 2.5191,
      "step": 1050
    },
    {
      "epoch": 0.31407407407407406,
      "grad_norm": 0.5124760866165161,
      "learning_rate": 0.00018431111111111113,
      "loss": 2.5626,
      "step": 1060
    },
    {
      "epoch": 0.31703703703703706,
      "grad_norm": 0.5052414536476135,
      "learning_rate": 0.00018416296296296298,
      "loss": 2.5573,
      "step": 1070
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.6296695470809937,
      "learning_rate": 0.00018401481481481483,
      "loss": 2.5774,
      "step": 1080
    },
    {
      "epoch": 0.32296296296296295,
      "grad_norm": 0.5750285983085632,
      "learning_rate": 0.00018386666666666667,
      "loss": 2.5621,
      "step": 1090
    },
    {
      "epoch": 0.32592592592592595,
      "grad_norm": 0.508026123046875,
      "learning_rate": 0.00018371851851851855,
      "loss": 2.5951,
      "step": 1100
    },
    {
      "epoch": 0.3288888888888889,
      "grad_norm": 0.4823842942714691,
      "learning_rate": 0.00018357037037037037,
      "loss": 2.5349,
      "step": 1110
    },
    {
      "epoch": 0.33185185185185184,
      "grad_norm": 0.4447552263736725,
      "learning_rate": 0.00018342222222222224,
      "loss": 2.5223,
      "step": 1120
    },
    {
      "epoch": 0.3348148148148148,
      "grad_norm": 0.5858858227729797,
      "learning_rate": 0.00018327407407407408,
      "loss": 2.565,
      "step": 1130
    },
    {
      "epoch": 0.3377777777777778,
      "grad_norm": 0.5836934447288513,
      "learning_rate": 0.00018312592592592596,
      "loss": 2.5807,
      "step": 1140
    },
    {
      "epoch": 0.34074074074074073,
      "grad_norm": 0.47007444500923157,
      "learning_rate": 0.00018297777777777778,
      "loss": 2.5586,
      "step": 1150
    },
    {
      "epoch": 0.3437037037037037,
      "grad_norm": 0.4842124283313751,
      "learning_rate": 0.00018282962962962965,
      "loss": 2.5131,
      "step": 1160
    },
    {
      "epoch": 0.3466666666666667,
      "grad_norm": 0.4994128346443176,
      "learning_rate": 0.0001826814814814815,
      "loss": 2.6058,
      "step": 1170
    },
    {
      "epoch": 0.3496296296296296,
      "grad_norm": 0.5816500782966614,
      "learning_rate": 0.00018253333333333334,
      "loss": 2.5592,
      "step": 1180
    },
    {
      "epoch": 0.35259259259259257,
      "grad_norm": 0.5543220639228821,
      "learning_rate": 0.0001823851851851852,
      "loss": 2.5695,
      "step": 1190
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 0.5005171298980713,
      "learning_rate": 0.00018223703703703706,
      "loss": 2.5388,
      "step": 1200
    },
    {
      "epoch": 0.3585185185185185,
      "grad_norm": 0.5601008534431458,
      "learning_rate": 0.00018208888888888888,
      "loss": 2.5626,
      "step": 1210
    },
    {
      "epoch": 0.36148148148148146,
      "grad_norm": 0.4648860991001129,
      "learning_rate": 0.00018194074074074076,
      "loss": 2.5188,
      "step": 1220
    },
    {
      "epoch": 0.36444444444444446,
      "grad_norm": 0.5102611780166626,
      "learning_rate": 0.0001817925925925926,
      "loss": 2.5506,
      "step": 1230
    },
    {
      "epoch": 0.3674074074074074,
      "grad_norm": 0.5268374085426331,
      "learning_rate": 0.00018164444444444447,
      "loss": 2.5358,
      "step": 1240
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 0.49688655138015747,
      "learning_rate": 0.0001814962962962963,
      "loss": 2.5333,
      "step": 1250
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 0.5843088626861572,
      "learning_rate": 0.00018134814814814817,
      "loss": 2.5538,
      "step": 1260
    },
    {
      "epoch": 0.3762962962962963,
      "grad_norm": 0.5350614786148071,
      "learning_rate": 0.0001812,
      "loss": 2.5654,
      "step": 1270
    },
    {
      "epoch": 0.37925925925925924,
      "grad_norm": 0.5340988039970398,
      "learning_rate": 0.00018105185185185186,
      "loss": 2.579,
      "step": 1280
    },
    {
      "epoch": 0.38222222222222224,
      "grad_norm": 0.5267772674560547,
      "learning_rate": 0.0001809037037037037,
      "loss": 2.547,
      "step": 1290
    },
    {
      "epoch": 0.3851851851851852,
      "grad_norm": 0.5857828259468079,
      "learning_rate": 0.00018075555555555558,
      "loss": 2.4993,
      "step": 1300
    },
    {
      "epoch": 0.38814814814814813,
      "grad_norm": 0.5475929379463196,
      "learning_rate": 0.00018060740740740743,
      "loss": 2.5535,
      "step": 1310
    },
    {
      "epoch": 0.39111111111111113,
      "grad_norm": 0.56365966796875,
      "learning_rate": 0.00018045925925925927,
      "loss": 2.583,
      "step": 1320
    },
    {
      "epoch": 0.3940740740740741,
      "grad_norm": 0.5002513527870178,
      "learning_rate": 0.00018031111111111112,
      "loss": 2.5697,
      "step": 1330
    },
    {
      "epoch": 0.397037037037037,
      "grad_norm": 0.5107060670852661,
      "learning_rate": 0.000180162962962963,
      "loss": 2.5269,
      "step": 1340
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.5154960751533508,
      "learning_rate": 0.0001800148148148148,
      "loss": 2.4921,
      "step": 1350
    },
    {
      "epoch": 0.40296296296296297,
      "grad_norm": 0.5223729610443115,
      "learning_rate": 0.00017986666666666668,
      "loss": 2.5731,
      "step": 1360
    },
    {
      "epoch": 0.4059259259259259,
      "grad_norm": 0.5136324167251587,
      "learning_rate": 0.00017971851851851853,
      "loss": 2.5535,
      "step": 1370
    },
    {
      "epoch": 0.4088888888888889,
      "grad_norm": 0.49840572476387024,
      "learning_rate": 0.00017957037037037038,
      "loss": 2.5621,
      "step": 1380
    },
    {
      "epoch": 0.41185185185185186,
      "grad_norm": 0.5313321352005005,
      "learning_rate": 0.00017942222222222222,
      "loss": 2.4995,
      "step": 1390
    },
    {
      "epoch": 0.4148148148148148,
      "grad_norm": 0.5582079887390137,
      "learning_rate": 0.0001792740740740741,
      "loss": 2.5269,
      "step": 1400
    },
    {
      "epoch": 0.4177777777777778,
      "grad_norm": 0.47032323479652405,
      "learning_rate": 0.00017912592592592594,
      "loss": 2.5533,
      "step": 1410
    },
    {
      "epoch": 0.42074074074074075,
      "grad_norm": 0.5444291830062866,
      "learning_rate": 0.0001789777777777778,
      "loss": 2.5793,
      "step": 1420
    },
    {
      "epoch": 0.4237037037037037,
      "grad_norm": 0.5078678727149963,
      "learning_rate": 0.00017882962962962963,
      "loss": 2.5783,
      "step": 1430
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 0.5904210805892944,
      "learning_rate": 0.00017868148148148148,
      "loss": 2.5186,
      "step": 1440
    },
    {
      "epoch": 0.42962962962962964,
      "grad_norm": 0.5272537469863892,
      "learning_rate": 0.00017853333333333335,
      "loss": 2.5872,
      "step": 1450
    },
    {
      "epoch": 0.4325925925925926,
      "grad_norm": 0.4913741648197174,
      "learning_rate": 0.0001783851851851852,
      "loss": 2.5306,
      "step": 1460
    },
    {
      "epoch": 0.43555555555555553,
      "grad_norm": 0.5588667988777161,
      "learning_rate": 0.00017823703703703705,
      "loss": 2.4606,
      "step": 1470
    },
    {
      "epoch": 0.43851851851851853,
      "grad_norm": 0.5204349756240845,
      "learning_rate": 0.0001780888888888889,
      "loss": 2.5515,
      "step": 1480
    },
    {
      "epoch": 0.4414814814814815,
      "grad_norm": 0.5503302812576294,
      "learning_rate": 0.00017794074074074074,
      "loss": 2.5299,
      "step": 1490
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 0.5940693616867065,
      "learning_rate": 0.0001777925925925926,
      "loss": 2.5095,
      "step": 1500
    },
    {
      "epoch": 0.4474074074074074,
      "grad_norm": 0.5629288554191589,
      "learning_rate": 0.00017764444444444446,
      "loss": 2.487,
      "step": 1510
    },
    {
      "epoch": 0.45037037037037037,
      "grad_norm": 0.5606803894042969,
      "learning_rate": 0.0001774962962962963,
      "loss": 2.5427,
      "step": 1520
    },
    {
      "epoch": 0.4533333333333333,
      "grad_norm": 0.5663149356842041,
      "learning_rate": 0.00017734814814814815,
      "loss": 2.5226,
      "step": 1530
    },
    {
      "epoch": 0.4562962962962963,
      "grad_norm": 0.554927408695221,
      "learning_rate": 0.0001772,
      "loss": 2.4946,
      "step": 1540
    },
    {
      "epoch": 0.45925925925925926,
      "grad_norm": 0.5474531054496765,
      "learning_rate": 0.00017705185185185187,
      "loss": 2.4929,
      "step": 1550
    },
    {
      "epoch": 0.4622222222222222,
      "grad_norm": 0.47661319375038147,
      "learning_rate": 0.00017690370370370372,
      "loss": 2.5181,
      "step": 1560
    },
    {
      "epoch": 0.4651851851851852,
      "grad_norm": 0.5712000131607056,
      "learning_rate": 0.00017675555555555556,
      "loss": 2.5416,
      "step": 1570
    },
    {
      "epoch": 0.46814814814814815,
      "grad_norm": 0.5680362582206726,
      "learning_rate": 0.0001766074074074074,
      "loss": 2.4774,
      "step": 1580
    },
    {
      "epoch": 0.4711111111111111,
      "grad_norm": 0.5421285629272461,
      "learning_rate": 0.00017645925925925926,
      "loss": 2.498,
      "step": 1590
    },
    {
      "epoch": 0.4740740740740741,
      "grad_norm": 0.5604150295257568,
      "learning_rate": 0.00017631111111111113,
      "loss": 2.4902,
      "step": 1600
    },
    {
      "epoch": 0.47703703703703704,
      "grad_norm": 0.5270572304725647,
      "learning_rate": 0.00017616296296296298,
      "loss": 2.5266,
      "step": 1610
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.5865124464035034,
      "learning_rate": 0.00017601481481481482,
      "loss": 2.5305,
      "step": 1620
    },
    {
      "epoch": 0.482962962962963,
      "grad_norm": 0.5561172366142273,
      "learning_rate": 0.00017586666666666667,
      "loss": 2.5613,
      "step": 1630
    },
    {
      "epoch": 0.48592592592592593,
      "grad_norm": 0.6066367626190186,
      "learning_rate": 0.00017571851851851851,
      "loss": 2.474,
      "step": 1640
    },
    {
      "epoch": 0.4888888888888889,
      "grad_norm": 0.49530014395713806,
      "learning_rate": 0.0001755703703703704,
      "loss": 2.505,
      "step": 1650
    },
    {
      "epoch": 0.4918518518518519,
      "grad_norm": 0.5276163816452026,
      "learning_rate": 0.00017542222222222223,
      "loss": 2.525,
      "step": 1660
    },
    {
      "epoch": 0.4948148148148148,
      "grad_norm": 0.59995037317276,
      "learning_rate": 0.00017527407407407408,
      "loss": 2.5334,
      "step": 1670
    },
    {
      "epoch": 0.49777777777777776,
      "grad_norm": 0.5855901837348938,
      "learning_rate": 0.00017512592592592593,
      "loss": 2.5214,
      "step": 1680
    },
    {
      "epoch": 0.5007407407407407,
      "grad_norm": 0.582145631313324,
      "learning_rate": 0.0001749777777777778,
      "loss": 2.4815,
      "step": 1690
    },
    {
      "epoch": 0.5037037037037037,
      "grad_norm": 0.49309206008911133,
      "learning_rate": 0.00017482962962962962,
      "loss": 2.4925,
      "step": 1700
    },
    {
      "epoch": 0.5066666666666667,
      "grad_norm": 0.5387232303619385,
      "learning_rate": 0.0001746814814814815,
      "loss": 2.5723,
      "step": 1710
    },
    {
      "epoch": 0.5096296296296297,
      "grad_norm": 0.6215090751647949,
      "learning_rate": 0.00017453333333333334,
      "loss": 2.5631,
      "step": 1720
    },
    {
      "epoch": 0.5125925925925926,
      "grad_norm": 0.520739734172821,
      "learning_rate": 0.00017438518518518518,
      "loss": 2.5176,
      "step": 1730
    },
    {
      "epoch": 0.5155555555555555,
      "grad_norm": 0.5500069856643677,
      "learning_rate": 0.00017423703703703703,
      "loss": 2.5595,
      "step": 1740
    },
    {
      "epoch": 0.5185185185185185,
      "grad_norm": 0.5245804786682129,
      "learning_rate": 0.0001740888888888889,
      "loss": 2.5251,
      "step": 1750
    },
    {
      "epoch": 0.5214814814814814,
      "grad_norm": 0.639426589012146,
      "learning_rate": 0.00017394074074074075,
      "loss": 2.5509,
      "step": 1760
    },
    {
      "epoch": 0.5244444444444445,
      "grad_norm": 0.591170072555542,
      "learning_rate": 0.0001737925925925926,
      "loss": 2.4542,
      "step": 1770
    },
    {
      "epoch": 0.5274074074074074,
      "grad_norm": 0.5089872479438782,
      "learning_rate": 0.00017364444444444444,
      "loss": 2.5681,
      "step": 1780
    },
    {
      "epoch": 0.5303703703703704,
      "grad_norm": 0.6132514476776123,
      "learning_rate": 0.00017349629629629632,
      "loss": 2.5096,
      "step": 1790
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.5256823897361755,
      "learning_rate": 0.00017334814814814814,
      "loss": 2.5489,
      "step": 1800
    },
    {
      "epoch": 0.5362962962962963,
      "grad_norm": 0.5730262398719788,
      "learning_rate": 0.0001732,
      "loss": 2.4998,
      "step": 1810
    },
    {
      "epoch": 0.5392592592592592,
      "grad_norm": 0.5843575596809387,
      "learning_rate": 0.00017305185185185185,
      "loss": 2.5461,
      "step": 1820
    },
    {
      "epoch": 0.5422222222222223,
      "grad_norm": 0.5474910736083984,
      "learning_rate": 0.0001729037037037037,
      "loss": 2.518,
      "step": 1830
    },
    {
      "epoch": 0.5451851851851852,
      "grad_norm": 0.56990647315979,
      "learning_rate": 0.00017275555555555555,
      "loss": 2.4475,
      "step": 1840
    },
    {
      "epoch": 0.5481481481481482,
      "grad_norm": 0.6107751727104187,
      "learning_rate": 0.00017260740740740742,
      "loss": 2.5353,
      "step": 1850
    },
    {
      "epoch": 0.5511111111111111,
      "grad_norm": 0.5529652833938599,
      "learning_rate": 0.00017245925925925927,
      "loss": 2.4625,
      "step": 1860
    },
    {
      "epoch": 0.554074074074074,
      "grad_norm": 0.600099503993988,
      "learning_rate": 0.0001723111111111111,
      "loss": 2.493,
      "step": 1870
    },
    {
      "epoch": 0.557037037037037,
      "grad_norm": 0.5579150915145874,
      "learning_rate": 0.00017216296296296296,
      "loss": 2.5673,
      "step": 1880
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.5313162207603455,
      "learning_rate": 0.00017201481481481483,
      "loss": 2.5041,
      "step": 1890
    },
    {
      "epoch": 0.562962962962963,
      "grad_norm": 0.5452969074249268,
      "learning_rate": 0.00017186666666666665,
      "loss": 2.5513,
      "step": 1900
    },
    {
      "epoch": 0.5659259259259259,
      "grad_norm": 0.5588946342468262,
      "learning_rate": 0.00017171851851851853,
      "loss": 2.5045,
      "step": 1910
    },
    {
      "epoch": 0.5688888888888889,
      "grad_norm": 0.5628202557563782,
      "learning_rate": 0.00017157037037037037,
      "loss": 2.4948,
      "step": 1920
    },
    {
      "epoch": 0.5718518518518518,
      "grad_norm": 0.516021728515625,
      "learning_rate": 0.00017142222222222224,
      "loss": 2.5138,
      "step": 1930
    },
    {
      "epoch": 0.5748148148148148,
      "grad_norm": 0.5599952340126038,
      "learning_rate": 0.00017127407407407406,
      "loss": 2.4807,
      "step": 1940
    },
    {
      "epoch": 0.5777777777777777,
      "grad_norm": 0.5833629369735718,
      "learning_rate": 0.00017112592592592594,
      "loss": 2.4854,
      "step": 1950
    },
    {
      "epoch": 0.5807407407407408,
      "grad_norm": 0.5750729441642761,
      "learning_rate": 0.00017097777777777778,
      "loss": 2.5392,
      "step": 1960
    },
    {
      "epoch": 0.5837037037037037,
      "grad_norm": 0.5582931637763977,
      "learning_rate": 0.00017082962962962963,
      "loss": 2.5252,
      "step": 1970
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 0.5889430046081543,
      "learning_rate": 0.00017068148148148148,
      "loss": 2.5247,
      "step": 1980
    },
    {
      "epoch": 0.5896296296296296,
      "grad_norm": 0.5151498317718506,
      "learning_rate": 0.00017053333333333335,
      "loss": 2.527,
      "step": 1990
    },
    {
      "epoch": 0.5925925925925926,
      "grad_norm": 0.5493420362472534,
      "learning_rate": 0.0001703851851851852,
      "loss": 2.5562,
      "step": 2000
    },
    {
      "epoch": 0.5955555555555555,
      "grad_norm": 0.6517258882522583,
      "learning_rate": 0.00017023703703703704,
      "loss": 2.5006,
      "step": 2010
    },
    {
      "epoch": 0.5985185185185186,
      "grad_norm": 0.5733888149261475,
      "learning_rate": 0.0001700888888888889,
      "loss": 2.482,
      "step": 2020
    },
    {
      "epoch": 0.6014814814814815,
      "grad_norm": 0.5706969499588013,
      "learning_rate": 0.00016994074074074076,
      "loss": 2.478,
      "step": 2030
    },
    {
      "epoch": 0.6044444444444445,
      "grad_norm": 0.5384312272071838,
      "learning_rate": 0.00016979259259259258,
      "loss": 2.5162,
      "step": 2040
    },
    {
      "epoch": 0.6074074074074074,
      "grad_norm": 0.4883746802806854,
      "learning_rate": 0.00016964444444444445,
      "loss": 2.4867,
      "step": 2050
    },
    {
      "epoch": 0.6103703703703703,
      "grad_norm": 0.48399776220321655,
      "learning_rate": 0.0001694962962962963,
      "loss": 2.5055,
      "step": 2060
    },
    {
      "epoch": 0.6133333333333333,
      "grad_norm": 0.5788560509681702,
      "learning_rate": 0.00016934814814814817,
      "loss": 2.4953,
      "step": 2070
    },
    {
      "epoch": 0.6162962962962963,
      "grad_norm": 0.5458008050918579,
      "learning_rate": 0.0001692,
      "loss": 2.5207,
      "step": 2080
    },
    {
      "epoch": 0.6192592592592593,
      "grad_norm": 0.5888392329216003,
      "learning_rate": 0.00016905185185185187,
      "loss": 2.5262,
      "step": 2090
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 0.5751901865005493,
      "learning_rate": 0.0001689037037037037,
      "loss": 2.5125,
      "step": 2100
    },
    {
      "epoch": 0.6251851851851852,
      "grad_norm": 0.5126862525939941,
      "learning_rate": 0.00016875555555555556,
      "loss": 2.4381,
      "step": 2110
    },
    {
      "epoch": 0.6281481481481481,
      "grad_norm": 0.5365992188453674,
      "learning_rate": 0.0001686074074074074,
      "loss": 2.4853,
      "step": 2120
    },
    {
      "epoch": 0.6311111111111111,
      "grad_norm": 0.561931848526001,
      "learning_rate": 0.00016845925925925928,
      "loss": 2.4965,
      "step": 2130
    },
    {
      "epoch": 0.6340740740740741,
      "grad_norm": 0.5584465861320496,
      "learning_rate": 0.0001683111111111111,
      "loss": 2.5365,
      "step": 2140
    },
    {
      "epoch": 0.6370370370370371,
      "grad_norm": 0.5985115766525269,
      "learning_rate": 0.00016816296296296297,
      "loss": 2.4822,
      "step": 2150
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.584868311882019,
      "learning_rate": 0.00016801481481481482,
      "loss": 2.4868,
      "step": 2160
    },
    {
      "epoch": 0.642962962962963,
      "grad_norm": 0.5507866144180298,
      "learning_rate": 0.0001678666666666667,
      "loss": 2.461,
      "step": 2170
    },
    {
      "epoch": 0.6459259259259259,
      "grad_norm": 0.5745070576667786,
      "learning_rate": 0.0001677185185185185,
      "loss": 2.4886,
      "step": 2180
    },
    {
      "epoch": 0.6488888888888888,
      "grad_norm": 0.533312976360321,
      "learning_rate": 0.00016757037037037038,
      "loss": 2.5135,
      "step": 2190
    },
    {
      "epoch": 0.6518518518518519,
      "grad_norm": 0.6284390687942505,
      "learning_rate": 0.00016742222222222223,
      "loss": 2.563,
      "step": 2200
    },
    {
      "epoch": 0.6548148148148148,
      "grad_norm": 0.6098790764808655,
      "learning_rate": 0.00016727407407407408,
      "loss": 2.509,
      "step": 2210
    },
    {
      "epoch": 0.6577777777777778,
      "grad_norm": 0.6533806324005127,
      "learning_rate": 0.00016712592592592592,
      "loss": 2.4662,
      "step": 2220
    },
    {
      "epoch": 0.6607407407407407,
      "grad_norm": 0.5905667543411255,
      "learning_rate": 0.0001669777777777778,
      "loss": 2.5383,
      "step": 2230
    },
    {
      "epoch": 0.6637037037037037,
      "grad_norm": 0.6362877488136292,
      "learning_rate": 0.00016682962962962964,
      "loss": 2.4599,
      "step": 2240
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.6392120718955994,
      "learning_rate": 0.0001666814814814815,
      "loss": 2.5098,
      "step": 2250
    },
    {
      "epoch": 0.6696296296296296,
      "grad_norm": 0.5785087943077087,
      "learning_rate": 0.00016653333333333333,
      "loss": 2.5032,
      "step": 2260
    },
    {
      "epoch": 0.6725925925925926,
      "grad_norm": 0.5562189221382141,
      "learning_rate": 0.0001663851851851852,
      "loss": 2.5015,
      "step": 2270
    },
    {
      "epoch": 0.6755555555555556,
      "grad_norm": 0.5677963495254517,
      "learning_rate": 0.00016623703703703703,
      "loss": 2.4728,
      "step": 2280
    },
    {
      "epoch": 0.6785185185185185,
      "grad_norm": 0.5773381590843201,
      "learning_rate": 0.0001660888888888889,
      "loss": 2.5174,
      "step": 2290
    },
    {
      "epoch": 0.6814814814814815,
      "grad_norm": 0.5186058282852173,
      "learning_rate": 0.00016594074074074075,
      "loss": 2.508,
      "step": 2300
    },
    {
      "epoch": 0.6844444444444444,
      "grad_norm": 0.5885648727416992,
      "learning_rate": 0.00016579259259259262,
      "loss": 2.5392,
      "step": 2310
    },
    {
      "epoch": 0.6874074074074074,
      "grad_norm": 0.5627070069313049,
      "learning_rate": 0.00016564444444444444,
      "loss": 2.5259,
      "step": 2320
    },
    {
      "epoch": 0.6903703703703704,
      "grad_norm": 0.5256631970405579,
      "learning_rate": 0.0001654962962962963,
      "loss": 2.454,
      "step": 2330
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 0.594304084777832,
      "learning_rate": 0.00016534814814814816,
      "loss": 2.5942,
      "step": 2340
    },
    {
      "epoch": 0.6962962962962963,
      "grad_norm": 0.6053695678710938,
      "learning_rate": 0.0001652,
      "loss": 2.5624,
      "step": 2350
    },
    {
      "epoch": 0.6992592592592592,
      "grad_norm": 0.5784710645675659,
      "learning_rate": 0.00016505185185185185,
      "loss": 2.5582,
      "step": 2360
    },
    {
      "epoch": 0.7022222222222222,
      "grad_norm": 0.5521157383918762,
      "learning_rate": 0.00016490370370370372,
      "loss": 2.4681,
      "step": 2370
    },
    {
      "epoch": 0.7051851851851851,
      "grad_norm": 0.5870792865753174,
      "learning_rate": 0.00016475555555555557,
      "loss": 2.4898,
      "step": 2380
    },
    {
      "epoch": 0.7081481481481482,
      "grad_norm": 0.5875150561332703,
      "learning_rate": 0.00016460740740740742,
      "loss": 2.5072,
      "step": 2390
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 0.6405550241470337,
      "learning_rate": 0.00016445925925925926,
      "loss": 2.4979,
      "step": 2400
    },
    {
      "epoch": 0.7140740740740741,
      "grad_norm": 0.6015918254852295,
      "learning_rate": 0.00016431111111111114,
      "loss": 2.501,
      "step": 2410
    },
    {
      "epoch": 0.717037037037037,
      "grad_norm": 0.7268711924552917,
      "learning_rate": 0.00016416296296296295,
      "loss": 2.5218,
      "step": 2420
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.5438575148582458,
      "learning_rate": 0.00016401481481481483,
      "loss": 2.5269,
      "step": 2430
    },
    {
      "epoch": 0.7229629629629629,
      "grad_norm": 0.5286772847175598,
      "learning_rate": 0.00016386666666666667,
      "loss": 2.4439,
      "step": 2440
    },
    {
      "epoch": 0.725925925925926,
      "grad_norm": 0.5618389844894409,
      "learning_rate": 0.00016371851851851852,
      "loss": 2.4961,
      "step": 2450
    },
    {
      "epoch": 0.7288888888888889,
      "grad_norm": 0.5326624512672424,
      "learning_rate": 0.00016357037037037037,
      "loss": 2.5146,
      "step": 2460
    },
    {
      "epoch": 0.7318518518518519,
      "grad_norm": 0.6017144322395325,
      "learning_rate": 0.00016342222222222224,
      "loss": 2.5427,
      "step": 2470
    },
    {
      "epoch": 0.7348148148148148,
      "grad_norm": 0.48224565386772156,
      "learning_rate": 0.00016327407407407409,
      "loss": 2.5243,
      "step": 2480
    },
    {
      "epoch": 0.7377777777777778,
      "grad_norm": 0.5940978527069092,
      "learning_rate": 0.00016312592592592593,
      "loss": 2.527,
      "step": 2490
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 0.5940923690795898,
      "learning_rate": 0.00016297777777777778,
      "loss": 2.5207,
      "step": 2500
    },
    {
      "epoch": 0.7437037037037038,
      "grad_norm": 0.4957677125930786,
      "learning_rate": 0.00016282962962962965,
      "loss": 2.467,
      "step": 2510
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 0.5597928166389465,
      "learning_rate": 0.00016268148148148147,
      "loss": 2.5175,
      "step": 2520
    },
    {
      "epoch": 0.7496296296296296,
      "grad_norm": 0.5426695346832275,
      "learning_rate": 0.00016253333333333334,
      "loss": 2.492,
      "step": 2530
    },
    {
      "epoch": 0.7525925925925926,
      "grad_norm": 0.6414598226547241,
      "learning_rate": 0.0001623851851851852,
      "loss": 2.5258,
      "step": 2540
    },
    {
      "epoch": 0.7555555555555555,
      "grad_norm": 0.5993574261665344,
      "learning_rate": 0.00016223703703703706,
      "loss": 2.5024,
      "step": 2550
    },
    {
      "epoch": 0.7585185185185185,
      "grad_norm": 0.5578445792198181,
      "learning_rate": 0.00016208888888888888,
      "loss": 2.4724,
      "step": 2560
    },
    {
      "epoch": 0.7614814814814815,
      "grad_norm": 0.5167445540428162,
      "learning_rate": 0.00016194074074074076,
      "loss": 2.4781,
      "step": 2570
    },
    {
      "epoch": 0.7644444444444445,
      "grad_norm": 0.5555174350738525,
      "learning_rate": 0.0001617925925925926,
      "loss": 2.5465,
      "step": 2580
    },
    {
      "epoch": 0.7674074074074074,
      "grad_norm": 0.5692397952079773,
      "learning_rate": 0.00016164444444444445,
      "loss": 2.5506,
      "step": 2590
    },
    {
      "epoch": 0.7703703703703704,
      "grad_norm": 0.5691792368888855,
      "learning_rate": 0.0001614962962962963,
      "loss": 2.5427,
      "step": 2600
    },
    {
      "epoch": 0.7733333333333333,
      "grad_norm": 0.5351101160049438,
      "learning_rate": 0.00016134814814814817,
      "loss": 2.5176,
      "step": 2610
    },
    {
      "epoch": 0.7762962962962963,
      "grad_norm": 0.5348200798034668,
      "learning_rate": 0.00016120000000000002,
      "loss": 2.5635,
      "step": 2620
    },
    {
      "epoch": 0.7792592592592592,
      "grad_norm": 0.5752236843109131,
      "learning_rate": 0.00016105185185185186,
      "loss": 2.5032,
      "step": 2630
    },
    {
      "epoch": 0.7822222222222223,
      "grad_norm": 0.5424786806106567,
      "learning_rate": 0.0001609037037037037,
      "loss": 2.4507,
      "step": 2640
    },
    {
      "epoch": 0.7851851851851852,
      "grad_norm": 0.5444689989089966,
      "learning_rate": 0.00016075555555555558,
      "loss": 2.4412,
      "step": 2650
    },
    {
      "epoch": 0.7881481481481482,
      "grad_norm": 0.5626870393753052,
      "learning_rate": 0.0001606074074074074,
      "loss": 2.4831,
      "step": 2660
    },
    {
      "epoch": 0.7911111111111111,
      "grad_norm": 0.5378977656364441,
      "learning_rate": 0.00016045925925925927,
      "loss": 2.5122,
      "step": 2670
    },
    {
      "epoch": 0.794074074074074,
      "grad_norm": 0.5653854608535767,
      "learning_rate": 0.00016031111111111112,
      "loss": 2.5159,
      "step": 2680
    },
    {
      "epoch": 0.797037037037037,
      "grad_norm": 0.5417589545249939,
      "learning_rate": 0.000160162962962963,
      "loss": 2.4657,
      "step": 2690
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.5326642394065857,
      "learning_rate": 0.0001600148148148148,
      "loss": 2.4461,
      "step": 2700
    },
    {
      "epoch": 0.802962962962963,
      "grad_norm": 0.6212728023529053,
      "learning_rate": 0.00015986666666666669,
      "loss": 2.4774,
      "step": 2710
    },
    {
      "epoch": 0.8059259259259259,
      "grad_norm": 0.5626786947250366,
      "learning_rate": 0.00015971851851851853,
      "loss": 2.4383,
      "step": 2720
    },
    {
      "epoch": 0.8088888888888889,
      "grad_norm": 0.5927907824516296,
      "learning_rate": 0.00015957037037037038,
      "loss": 2.4942,
      "step": 2730
    },
    {
      "epoch": 0.8118518518518518,
      "grad_norm": 0.563608705997467,
      "learning_rate": 0.00015942222222222222,
      "loss": 2.4789,
      "step": 2740
    },
    {
      "epoch": 0.8148148148148148,
      "grad_norm": 0.6373249888420105,
      "learning_rate": 0.0001592740740740741,
      "loss": 2.5221,
      "step": 2750
    },
    {
      "epoch": 0.8177777777777778,
      "grad_norm": 0.5481829047203064,
      "learning_rate": 0.00015912592592592592,
      "loss": 2.4396,
      "step": 2760
    },
    {
      "epoch": 0.8207407407407408,
      "grad_norm": 0.5743747353553772,
      "learning_rate": 0.0001589777777777778,
      "loss": 2.4845,
      "step": 2770
    },
    {
      "epoch": 0.8237037037037037,
      "grad_norm": 0.6300185918807983,
      "learning_rate": 0.00015882962962962964,
      "loss": 2.5148,
      "step": 2780
    },
    {
      "epoch": 0.8266666666666667,
      "grad_norm": 0.5679615139961243,
      "learning_rate": 0.0001586814814814815,
      "loss": 2.5031,
      "step": 2790
    },
    {
      "epoch": 0.8296296296296296,
      "grad_norm": 0.5980815291404724,
      "learning_rate": 0.00015853333333333333,
      "loss": 2.4289,
      "step": 2800
    },
    {
      "epoch": 0.8325925925925926,
      "grad_norm": 0.5685645937919617,
      "learning_rate": 0.0001583851851851852,
      "loss": 2.4628,
      "step": 2810
    },
    {
      "epoch": 0.8355555555555556,
      "grad_norm": 0.5320000052452087,
      "learning_rate": 0.00015823703703703705,
      "loss": 2.444,
      "step": 2820
    },
    {
      "epoch": 0.8385185185185186,
      "grad_norm": 0.5394196510314941,
      "learning_rate": 0.0001580888888888889,
      "loss": 2.5437,
      "step": 2830
    },
    {
      "epoch": 0.8414814814814815,
      "grad_norm": 0.516316294670105,
      "learning_rate": 0.00015794074074074074,
      "loss": 2.4887,
      "step": 2840
    },
    {
      "epoch": 0.8444444444444444,
      "grad_norm": 0.5557376742362976,
      "learning_rate": 0.00015779259259259261,
      "loss": 2.5183,
      "step": 2850
    },
    {
      "epoch": 0.8474074074074074,
      "grad_norm": 0.5955705046653748,
      "learning_rate": 0.00015764444444444446,
      "loss": 2.468,
      "step": 2860
    },
    {
      "epoch": 0.8503703703703703,
      "grad_norm": 0.569079577922821,
      "learning_rate": 0.0001574962962962963,
      "loss": 2.432,
      "step": 2870
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 0.5679547190666199,
      "learning_rate": 0.00015734814814814815,
      "loss": 2.4669,
      "step": 2880
    },
    {
      "epoch": 0.8562962962962963,
      "grad_norm": 0.6560715436935425,
      "learning_rate": 0.00015720000000000003,
      "loss": 2.4638,
      "step": 2890
    },
    {
      "epoch": 0.8592592592592593,
      "grad_norm": 0.6482306122779846,
      "learning_rate": 0.00015705185185185185,
      "loss": 2.5169,
      "step": 2900
    },
    {
      "epoch": 0.8622222222222222,
      "grad_norm": 0.6091601848602295,
      "learning_rate": 0.00015690370370370372,
      "loss": 2.4568,
      "step": 2910
    },
    {
      "epoch": 0.8651851851851852,
      "grad_norm": 0.6090559363365173,
      "learning_rate": 0.00015675555555555557,
      "loss": 2.4916,
      "step": 2920
    },
    {
      "epoch": 0.8681481481481481,
      "grad_norm": 0.6311323046684265,
      "learning_rate": 0.00015660740740740744,
      "loss": 2.499,
      "step": 2930
    },
    {
      "epoch": 0.8711111111111111,
      "grad_norm": 0.5675535798072815,
      "learning_rate": 0.00015645925925925926,
      "loss": 2.5426,
      "step": 2940
    },
    {
      "epoch": 0.8740740740740741,
      "grad_norm": 0.579653263092041,
      "learning_rate": 0.00015631111111111113,
      "loss": 2.5158,
      "step": 2950
    },
    {
      "epoch": 0.8770370370370371,
      "grad_norm": 0.5264046788215637,
      "learning_rate": 0.00015616296296296298,
      "loss": 2.4544,
      "step": 2960
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.5778030157089233,
      "learning_rate": 0.00015601481481481482,
      "loss": 2.4797,
      "step": 2970
    },
    {
      "epoch": 0.882962962962963,
      "grad_norm": 0.5998910069465637,
      "learning_rate": 0.00015586666666666667,
      "loss": 2.5184,
      "step": 2980
    },
    {
      "epoch": 0.8859259259259259,
      "grad_norm": 0.5402879118919373,
      "learning_rate": 0.00015571851851851854,
      "loss": 2.5035,
      "step": 2990
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 0.6006041169166565,
      "learning_rate": 0.0001555703703703704,
      "loss": 2.4984,
      "step": 3000
    },
    {
      "epoch": 0.8918518518518519,
      "grad_norm": 0.5538323521614075,
      "learning_rate": 0.00015542222222222224,
      "loss": 2.4591,
      "step": 3010
    },
    {
      "epoch": 0.8948148148148148,
      "grad_norm": 0.5369256138801575,
      "learning_rate": 0.00015527407407407408,
      "loss": 2.5662,
      "step": 3020
    },
    {
      "epoch": 0.8977777777777778,
      "grad_norm": 0.6436570286750793,
      "learning_rate": 0.00015512592592592593,
      "loss": 2.4605,
      "step": 3030
    },
    {
      "epoch": 0.9007407407407407,
      "grad_norm": 0.5211138725280762,
      "learning_rate": 0.00015497777777777777,
      "loss": 2.4669,
      "step": 3040
    },
    {
      "epoch": 0.9037037037037037,
      "grad_norm": 0.5596984028816223,
      "learning_rate": 0.00015482962962962965,
      "loss": 2.4655,
      "step": 3050
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 0.5837700963020325,
      "learning_rate": 0.0001546814814814815,
      "loss": 2.4694,
      "step": 3060
    },
    {
      "epoch": 0.9096296296296297,
      "grad_norm": 0.4673502445220947,
      "learning_rate": 0.00015453333333333334,
      "loss": 2.4808,
      "step": 3070
    },
    {
      "epoch": 0.9125925925925926,
      "grad_norm": 0.5638529062271118,
      "learning_rate": 0.00015438518518518519,
      "loss": 2.4991,
      "step": 3080
    },
    {
      "epoch": 0.9155555555555556,
      "grad_norm": 0.6276779174804688,
      "learning_rate": 0.00015423703703703706,
      "loss": 2.4744,
      "step": 3090
    },
    {
      "epoch": 0.9185185185185185,
      "grad_norm": 0.5858723521232605,
      "learning_rate": 0.0001540888888888889,
      "loss": 2.5051,
      "step": 3100
    },
    {
      "epoch": 0.9214814814814815,
      "grad_norm": 0.5960589051246643,
      "learning_rate": 0.00015394074074074075,
      "loss": 2.4872,
      "step": 3110
    },
    {
      "epoch": 0.9244444444444444,
      "grad_norm": 0.5339594483375549,
      "learning_rate": 0.0001537925925925926,
      "loss": 2.4666,
      "step": 3120
    },
    {
      "epoch": 0.9274074074074075,
      "grad_norm": 0.6239520311355591,
      "learning_rate": 0.00015364444444444444,
      "loss": 2.4858,
      "step": 3130
    },
    {
      "epoch": 0.9303703703703704,
      "grad_norm": 0.6121558547019958,
      "learning_rate": 0.0001534962962962963,
      "loss": 2.4776,
      "step": 3140
    },
    {
      "epoch": 0.9333333333333333,
      "grad_norm": 0.5478872656822205,
      "learning_rate": 0.00015334814814814816,
      "loss": 2.4904,
      "step": 3150
    },
    {
      "epoch": 0.9362962962962963,
      "grad_norm": 0.597148597240448,
      "learning_rate": 0.0001532,
      "loss": 2.4869,
      "step": 3160
    },
    {
      "epoch": 0.9392592592592592,
      "grad_norm": 0.5738146901130676,
      "learning_rate": 0.00015305185185185186,
      "loss": 2.4978,
      "step": 3170
    },
    {
      "epoch": 0.9422222222222222,
      "grad_norm": 0.6657956838607788,
      "learning_rate": 0.0001529037037037037,
      "loss": 2.4841,
      "step": 3180
    },
    {
      "epoch": 0.9451851851851852,
      "grad_norm": 0.5128861665725708,
      "learning_rate": 0.00015275555555555558,
      "loss": 2.5289,
      "step": 3190
    },
    {
      "epoch": 0.9481481481481482,
      "grad_norm": 0.6678973436355591,
      "learning_rate": 0.00015260740740740742,
      "loss": 2.4691,
      "step": 3200
    },
    {
      "epoch": 0.9511111111111111,
      "grad_norm": 0.5422954559326172,
      "learning_rate": 0.00015245925925925927,
      "loss": 2.5446,
      "step": 3210
    },
    {
      "epoch": 0.9540740740740741,
      "grad_norm": 0.6429051756858826,
      "learning_rate": 0.00015231111111111111,
      "loss": 2.4884,
      "step": 3220
    },
    {
      "epoch": 0.957037037037037,
      "grad_norm": 0.6255701780319214,
      "learning_rate": 0.00015216296296296296,
      "loss": 2.5036,
      "step": 3230
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.6273384690284729,
      "learning_rate": 0.00015201481481481483,
      "loss": 2.4633,
      "step": 3240
    },
    {
      "epoch": 0.9629629629629629,
      "grad_norm": 0.529193639755249,
      "learning_rate": 0.00015186666666666668,
      "loss": 2.4986,
      "step": 3250
    },
    {
      "epoch": 0.965925925925926,
      "grad_norm": 0.5956535935401917,
      "learning_rate": 0.00015171851851851853,
      "loss": 2.4607,
      "step": 3260
    },
    {
      "epoch": 0.9688888888888889,
      "grad_norm": 0.5837367177009583,
      "learning_rate": 0.00015157037037037037,
      "loss": 2.4765,
      "step": 3270
    },
    {
      "epoch": 0.9718518518518519,
      "grad_norm": 0.5836264491081238,
      "learning_rate": 0.00015142222222222222,
      "loss": 2.4858,
      "step": 3280
    },
    {
      "epoch": 0.9748148148148148,
      "grad_norm": 0.5536299347877502,
      "learning_rate": 0.00015127407407407407,
      "loss": 2.5165,
      "step": 3290
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 0.6081740260124207,
      "learning_rate": 0.00015112592592592594,
      "loss": 2.5125,
      "step": 3300
    },
    {
      "epoch": 0.9807407407407407,
      "grad_norm": 0.5883432030677795,
      "learning_rate": 0.00015097777777777779,
      "loss": 2.4733,
      "step": 3310
    },
    {
      "epoch": 0.9837037037037037,
      "grad_norm": 0.6223667860031128,
      "learning_rate": 0.00015082962962962963,
      "loss": 2.4038,
      "step": 3320
    },
    {
      "epoch": 0.9866666666666667,
      "grad_norm": 0.5576180219650269,
      "learning_rate": 0.00015068148148148148,
      "loss": 2.4616,
      "step": 3330
    },
    {
      "epoch": 0.9896296296296296,
      "grad_norm": 0.6024103164672852,
      "learning_rate": 0.00015053333333333335,
      "loss": 2.4921,
      "step": 3340
    },
    {
      "epoch": 0.9925925925925926,
      "grad_norm": 0.5758699774742126,
      "learning_rate": 0.0001503851851851852,
      "loss": 2.5145,
      "step": 3350
    },
    {
      "epoch": 0.9955555555555555,
      "grad_norm": 0.6660250425338745,
      "learning_rate": 0.00015023703703703704,
      "loss": 2.4638,
      "step": 3360
    },
    {
      "epoch": 0.9985185185185185,
      "grad_norm": 0.5874970555305481,
      "learning_rate": 0.0001500888888888889,
      "loss": 2.5213,
      "step": 3370
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.452345132827759,
      "eval_runtime": 2503.9974,
      "eval_samples_per_second": 2.396,
      "eval_steps_per_second": 2.396,
      "step": 3375
    },
    {
      "epoch": 1.0014814814814814,
      "grad_norm": 0.6100268959999084,
      "learning_rate": 0.00014994074074074074,
      "loss": 2.4582,
      "step": 3380
    },
    {
      "epoch": 1.0044444444444445,
      "grad_norm": 0.6194929480552673,
      "learning_rate": 0.00014979259259259258,
      "loss": 2.4029,
      "step": 3390
    },
    {
      "epoch": 1.0074074074074073,
      "grad_norm": 0.5693188309669495,
      "learning_rate": 0.00014964444444444446,
      "loss": 2.4078,
      "step": 3400
    },
    {
      "epoch": 1.0103703703703704,
      "grad_norm": 0.5982259511947632,
      "learning_rate": 0.0001494962962962963,
      "loss": 2.4646,
      "step": 3410
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 0.643567681312561,
      "learning_rate": 0.00014934814814814815,
      "loss": 2.4171,
      "step": 3420
    },
    {
      "epoch": 1.0162962962962963,
      "grad_norm": 0.5247811675071716,
      "learning_rate": 0.0001492,
      "loss": 2.3726,
      "step": 3430
    },
    {
      "epoch": 1.0192592592592593,
      "grad_norm": 0.5324362516403198,
      "learning_rate": 0.00014905185185185187,
      "loss": 2.4592,
      "step": 3440
    },
    {
      "epoch": 1.0222222222222221,
      "grad_norm": 0.666907548904419,
      "learning_rate": 0.00014890370370370371,
      "loss": 2.4026,
      "step": 3450
    },
    {
      "epoch": 1.0251851851851852,
      "grad_norm": 0.5540719628334045,
      "learning_rate": 0.00014875555555555556,
      "loss": 2.4401,
      "step": 3460
    },
    {
      "epoch": 1.0281481481481483,
      "grad_norm": 0.6429390907287598,
      "learning_rate": 0.0001486074074074074,
      "loss": 2.3679,
      "step": 3470
    },
    {
      "epoch": 1.031111111111111,
      "grad_norm": 0.6116220355033875,
      "learning_rate": 0.00014845925925925928,
      "loss": 2.4251,
      "step": 3480
    },
    {
      "epoch": 1.0340740740740741,
      "grad_norm": 0.7073956727981567,
      "learning_rate": 0.0001483111111111111,
      "loss": 2.4139,
      "step": 3490
    },
    {
      "epoch": 1.037037037037037,
      "grad_norm": 0.5615595579147339,
      "learning_rate": 0.00014816296296296297,
      "loss": 2.4216,
      "step": 3500
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.628553569316864,
      "learning_rate": 0.00014801481481481482,
      "loss": 2.385,
      "step": 3510
    },
    {
      "epoch": 1.0429629629629629,
      "grad_norm": 0.5664941072463989,
      "learning_rate": 0.00014786666666666666,
      "loss": 2.4107,
      "step": 3520
    },
    {
      "epoch": 1.045925925925926,
      "grad_norm": 0.5984950065612793,
      "learning_rate": 0.0001477185185185185,
      "loss": 2.4373,
      "step": 3530
    },
    {
      "epoch": 1.048888888888889,
      "grad_norm": 0.7535315155982971,
      "learning_rate": 0.00014757037037037038,
      "loss": 2.4466,
      "step": 3540
    },
    {
      "epoch": 1.0518518518518518,
      "grad_norm": 0.5876790881156921,
      "learning_rate": 0.00014742222222222223,
      "loss": 2.4697,
      "step": 3550
    },
    {
      "epoch": 1.0548148148148149,
      "grad_norm": 0.6691854596138,
      "learning_rate": 0.00014727407407407408,
      "loss": 2.4252,
      "step": 3560
    },
    {
      "epoch": 1.0577777777777777,
      "grad_norm": 0.6384322643280029,
      "learning_rate": 0.00014712592592592592,
      "loss": 2.4294,
      "step": 3570
    },
    {
      "epoch": 1.0607407407407408,
      "grad_norm": 0.6205068826675415,
      "learning_rate": 0.0001469777777777778,
      "loss": 2.4495,
      "step": 3580
    },
    {
      "epoch": 1.0637037037037036,
      "grad_norm": 0.6518883109092712,
      "learning_rate": 0.00014682962962962962,
      "loss": 2.4487,
      "step": 3590
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 0.5995984673500061,
      "learning_rate": 0.0001466814814814815,
      "loss": 2.4421,
      "step": 3600
    },
    {
      "epoch": 1.0696296296296297,
      "grad_norm": 0.5678324699401855,
      "learning_rate": 0.00014653333333333334,
      "loss": 2.4167,
      "step": 3610
    },
    {
      "epoch": 1.0725925925925925,
      "grad_norm": 0.5994903445243835,
      "learning_rate": 0.0001463851851851852,
      "loss": 2.4151,
      "step": 3620
    },
    {
      "epoch": 1.0755555555555556,
      "grad_norm": 0.5682450532913208,
      "learning_rate": 0.00014623703703703703,
      "loss": 2.4211,
      "step": 3630
    },
    {
      "epoch": 1.0785185185185184,
      "grad_norm": 0.6369513273239136,
      "learning_rate": 0.0001460888888888889,
      "loss": 2.4802,
      "step": 3640
    },
    {
      "epoch": 1.0814814814814815,
      "grad_norm": 0.6417917013168335,
      "learning_rate": 0.00014594074074074075,
      "loss": 2.4403,
      "step": 3650
    },
    {
      "epoch": 1.0844444444444445,
      "grad_norm": 0.6172380447387695,
      "learning_rate": 0.0001457925925925926,
      "loss": 2.4542,
      "step": 3660
    },
    {
      "epoch": 1.0874074074074074,
      "grad_norm": 0.5362762212753296,
      "learning_rate": 0.00014564444444444444,
      "loss": 2.4178,
      "step": 3670
    },
    {
      "epoch": 1.0903703703703704,
      "grad_norm": 0.5704454183578491,
      "learning_rate": 0.0001454962962962963,
      "loss": 2.4229,
      "step": 3680
    },
    {
      "epoch": 1.0933333333333333,
      "grad_norm": 0.575042724609375,
      "learning_rate": 0.00014534814814814813,
      "loss": 2.3875,
      "step": 3690
    },
    {
      "epoch": 1.0962962962962963,
      "grad_norm": 0.6792209148406982,
      "learning_rate": 0.0001452,
      "loss": 2.3882,
      "step": 3700
    },
    {
      "epoch": 1.0992592592592592,
      "grad_norm": 0.5901994705200195,
      "learning_rate": 0.00014505185185185185,
      "loss": 2.4399,
      "step": 3710
    },
    {
      "epoch": 1.1022222222222222,
      "grad_norm": 0.6282414793968201,
      "learning_rate": 0.00014490370370370373,
      "loss": 2.4077,
      "step": 3720
    },
    {
      "epoch": 1.1051851851851853,
      "grad_norm": 0.5524209141731262,
      "learning_rate": 0.00014475555555555554,
      "loss": 2.4684,
      "step": 3730
    },
    {
      "epoch": 1.108148148148148,
      "grad_norm": 0.6206862926483154,
      "learning_rate": 0.00014460740740740742,
      "loss": 2.4614,
      "step": 3740
    },
    {
      "epoch": 1.1111111111111112,
      "grad_norm": 0.6804226636886597,
      "learning_rate": 0.00014445925925925926,
      "loss": 2.4289,
      "step": 3750
    }
  ],
  "logging_steps": 10,
  "max_steps": 13500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 250,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 7.2579192717312e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
